# Clear environment
rm(list = ls())
graphics.off()
# Set working directory
#setwd("YOUR OWN CHOICE HERE")
setwd("~/Documents/UNI/2022/Honours/BIOL6502/Data/sarahfish")
# Obtain required functions from 'functions' subfolder
sapply(list.files("./Functions", pattern="\\.R", full.names=TRUE), source)
return(ts_list)
# Clear environment
rm(list = ls())
graphics.off()
# Set working directory
#setwd("YOUR OWN CHOICE HERE")
setwd("~/Documents/UNI/2022/Honours/BIOL6502/Data/sarahfish")
# Obtain required functions from 'functions' subfolder
sapply(list.files("./Functions", pattern="\\.R", full.names=TRUE), source)
# Load packages
install.packages(c("mgcv", "vegan", "lme4", "nlme",
"DHARMa", "merTools", "shape",
"multcomp", "maptools", "sp",
"divDyn", "plotrix", "raster",
"rgeos", "fun", "analogue",
"brms", "data.table", "dplyr",
"tidyverse", "funrar"))
# RivFishTIME data
time_series_data <-read.csv("inputs/1873_2_RivFishTIME_TimeseriesTable.csv")
survey_data <- read.csv("inputs/1873_2_RivFishTIME_SurveyTable.csv")
# Remove spaces from the quarter column
survey_data <- survey_data %>%
mutate(Quarter = str_replace_all(Quarter, " ", ""))
library(dplyr)
# Remove spaces from the quarter column
survey_data <- survey_data %>%
mutate(Quarter = str_replace_all(Quarter, " ", ""))
library(tidyverse)
# Remove spaces from the quarter column
survey_data <- survey_data %>%
mutate(Quarter = str_replace_all(Quarter, " ", ""))
# Table of timeseries with min 10 consecutive yearly surveys and 2 species
pilot_ts_table <- survey_data %>% group_by(TimeSeriesID) %>%
summarise(year_count = length(unique(Year)), first_year = min(Year),
last_year = max(Year), first_to_last = last_year - first_year,
full_data = (year_count == first_to_last),
species_count = length(unique(Species)),
quarter_count = length(unique(Quarter))) %>%
filter(full_data == TRUE, year_count >=10, species_count > 1)
# Modify survey data to only include timeseries from pilot_ts_table
pilot_survey_data <- survey_data[survey_data$TimeSeriesID %in%
pilot_ts_list$TimeSeriesID,]
# Modify timeseries to only include time series from pilot_ts_table
pilot_ts_data <- time_series_data[time_series_data$TimeSeriesID %in%
pilot_ts_list$TimeSeriesID,]
# Remove unused datasets
rm(survey_data, time_series_data, pilot_ts_table)
# RivFishTIME data
time_series_data <-read.csv("inputs/1873_2_RivFishTIME_TimeseriesTable.csv")
survey_data <- read.csv("inputs/1873_2_RivFishTIME_SurveyTable.csv")
# Remove spaces from the quarter column
survey_data <- survey_data %>%
mutate(Quarter = str_replace_all(Quarter, " ", ""))
# Table of timeseries with min 10 consecutive yearly surveys and 2 species
pilot_ts_table <- survey_data %>% group_by(TimeSeriesID) %>%
summarise(year_count = length(unique(Year)), first_year = min(Year),
last_year = max(Year), first_to_last = last_year - first_year,
full_data = (year_count == first_to_last),
species_count = length(unique(Species)),
quarter_count = length(unique(Quarter))) %>%
filter(full_data == TRUE, year_count >=10, species_count > 1)
# Modify survey data to only include timeseries from pilot_ts_table
pilot_survey_data <- survey_data[survey_data$TimeSeriesID %in%
pilot_ts_list$TimeSeriesID,]
# Modify survey data to only include timeseries from pilot_ts_table
pilot_survey_data <- survey_data[survey_data$TimeSeriesID %in%
pilot_ts_table$TimeSeriesID,]
# Modify timeseries to only include time series from pilot_ts_table
pilot_ts_data <- time_series_data[time_series_data$TimeSeriesID %in%
pilot_ts_table$TimeSeriesID,]
# Remove unused datasets
rm(survey_data, time_series_data, pilot_ts_table)
# Create a list of abundance matrices using matrix maker function
tax_matrix_list <- taxonomic_matrix_maker(pilot_survey_data,
pilot_ts_data)
library(funrar)
# Create a list of abundance matrices using matrix maker function
tax_matrix_list <- taxonomic_matrix_maker(pilot_survey_data,
pilot_ts_data)
# Apply framework using taxonomic novelty detection function
pilot_tax_results <- tax_novelty_detection(tax_matrix_list)
# Convert results list to a dataframe
pilot_tax_results.df <- do.call("rbind", pilot_tax_results)
View(pilot_tax_results.df)
# Novelty count result and store as dataframe
tax_novelty_table <- as.data.frame.matrix(table(pilot_tax_results.df$site, pilot_tax_results.df$cat))
View(tax_novelty_table)
# Loop over to get success failure columns and site ID
tax_comm_state <- lapply(1:ncol(tax_novelty_table), function(x) {
data.frame(site = rownames(tax_novelty_table),
success = tax_novelty_table[,x],
failure = rowSums(tax_novelty_table[,-x]))
})
names(tax_comm_state) <- colnames(tax_novelty_table)
View(tax_comm_state)
tax_prob_results <- lapply(tax_comm_state, function(x){
glm(cbind(success, failure) ~ 1,
data=x, family=binomial)
})
# Shows log odds results
lapply(tax_prob_results, summary)
View(tax_prob_results)
plogis(c(tax_prob_results$back))
plogis(c(tax_prob_results$back[1]))
# Convert to normal probability
state.prob <- lapply(tax_prob_results[[1]], plogis)
# Convert to normal probability
state.prob <- lapply(tax_prob_results[1], plogis)
# Convert to normal probability
state.prob <- lapply(tax_prob_results[[[]],[["coefficients"]]], plogis)
tax_prob_results[["back"]][["coefficients"]]]
plogis(tax_prob_results[x[1]])
plogis(tax_prob_results["x"[1]])
plogis(tax_prob_results[x,[1]])
# Novel
novel.prob <- plogis(tax_prob_results[1[1]])
# Novel
back.prob <- plogis(tax_prob_results[["back"]][["coefficients"]])
instant.prob <- plogis(tax_prob_results[["instant"]][["coefficients"]])
cumul.prob <- plogis(tax_prob_results[["cumul"]][["coefficients"]])
novel.prob <- plogis(tax_prob_results[["novel"]][["coefficients"]])
View(taxonomic_matrix_maker)
View(tax_novelty_detection)
View(pilot_tax_results)
View(pilot_tax_results.df)
View(tax_novelty_table)
View(tax_comm_state)
View(tax_prob_results)
# Shows log odds results
lapply(tax_prob_results, summary)
# Rudimentary probability conversion
back.prob <- plogis(tax_prob_results[["back"]][["coefficients"]])
source("~/Documents/UNI/2022/Honours/BIOL6502/Data/sarahfish/Pilot_Analysis.R")
